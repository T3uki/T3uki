def readData(DataSet):
    for data in DataSet:
        text = data['text']
        
        types = []
        for trigger in data['event_chain']:
            type = trigger['event_type']
            types.append(type)
        
        labels = [float(1) if unique_labels[i] in types else float(0) for i in range(len(unique_labels))]
        
        yield {'texts':text,'labels':labels}
        from paddlenlp.datasets import load_dataset

# load_dataset()创建数据集
train_ds = load_dataset(readData, DataSet=train_dataset, lazy=False) 
valid_ds = load_dataset(readData, DataSet=val_dataset, lazy=False)

# lazy=False，数据集返回为MapDataset类型
print("数据类型:", type(train_ds))

# labels为One-hot标签
print("训练集样例:", train_ds[0])
print("测试集样例:", valid_ds[0])

from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer

model_name = "ernie-3.0-base-zh"
num_classes = len(unique_labels)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)
tokenizer = AutoTokenizer.from_pretrained(model_name)
import numpy as np

def count_seq_lengths(dataset):
    lengths = []
    for data in dataset:
        res = tokenizer(data['texts'])
        lengths.append(len(res['input_ids']))
    
    lengths.sort()
    ls = np.array(lengths)
    n, bins, patches = plt.hist(ls,50,range=(0,500),facecolor='g')

count_seq_lengths(train_ds)

# max_legnth调整为256

import functools
import numpy as np

from paddle.io import DataLoader, BatchSampler
from paddlenlp.data import DataCollatorWithPadding

# 数据预处理函数，利用分词器将文本转化为整数序列
def preprocess_function(examples, tokenizer, max_seq_length):
    result = tokenizer(text=examples["texts"], max_seq_len=max_seq_length)
    result["labels"] = examples["labels"]
    return result

trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=256)
train_ds = train_ds.map(trans_func)
valid_ds = valid_ds.map(trans_func)

# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠
collate_fn = DataCollatorWithPadding(tokenizer)

# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader
train_batch_sampler = BatchSampler(train_ds, batch_size=64, shuffle=True)
valid_batch_sampler = BatchSampler(valid_ds, batch_size=64, shuffle=False)
train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)
valid_data_loader = DataLoader(dataset=valid_ds, batch_sampler=valid_batch_sampler, collate_fn=collate_fn)

import paddle
import time
import paddle.nn.functional as F
from sklearn.metrics import roc_auc_score, f1_score

optimizer = paddle.optimizer.AdamW(learning_rate=1e-4, parameters=model.parameters())
criterion = paddle.nn.BCEWithLogitsLoss()

# from eval import evaluate
import os 
epochs = 15 # 训练轮次
ckpt_dir = "ernie_ckpt" #训练过程中保存模型参数的文件夹

global_step = 0 #迭代次数
tic_train = time.time()
best_f1_score = 0

for epoch in range(1, epochs + 1):
    y_prob,y_true = None,None
    for step, batch in enumerate(train_data_loader, start=1):
        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']

        # 计算模型输出、损失函数值、分类概率值、准确率、f1分数
        logits = model(input_ids, token_type_ids)
        loss = criterion(logits, labels)
        probs = F.sigmoid(logits)

        loss.backward()
        optimizer.step()
        optimizer.clear_grad()

        if y_prob is not None:
            y_prob = np.append(y_prob,probs.numpy(),axis=0)
        else:
            y_prob = probs.numpy()
        if y_true is not None:
            y_true = np.append(y_true,labels.numpy(),axis=0)
        else:
            y_true = labels.numpy()
        
    auc = roc_auc_score(y_score=y_prob,y_true=y_true,average='micro')
    
    # f1
    best_score = 0
    for threshold in [i*0.01 for i in range(100)]:
        y_pred = y_prob > threshold
        score = f1_score(y_pred=y_pred,y_true=y_true,average='micro')
        if score > best_score:
            best_score = score
    
    print("train performance: epoch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s"
         %(epoch,loss,auc,best_score,10/(time.time()-tic_train))
         )
    
    tic_train = time.time()

    model.eval()    
    y_prob,y_true = None,None
    for step,batch in enumerate(valid_data_loader,start=1):
        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']
        output = model(input_ids,token_type_ids)
        probs = F.sigmoid(output)

        if y_prob is not None:
            y_prob = np.append(y_prob,probs.numpy(),axis=0)
        else:
            y_prob = probs.numpy()
        if y_true is not None:
            y_true = np.append(y_true,labels.numpy(),axis=0)
        else:
            y_true = labels.numpy()
        
    # f1
    best_score = 0
    for threshold in [i*0.01 for i in range(100)]:
        y_pred = y_prob > threshold
        score = f1_score(y_pred=y_pred,y_true=y_true,average='micro')
        if score > best_score:
            best_score = score

    print("valid f1 performance: %.5f"%(best_score))
            
    if not os.path.exists(ckpt_dir):
        os.makedirs(ckpt_dir)

    if best_score > best_f1_score:
        best_f1_score = best_score
        model.save_pretrained(ckpt_dir)
        tokenizer.save_pretrained(ckpt_dir)
